{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IOI Circuit Replication Validation\n",
    "\n",
    "**End-to-End Validation Against \"Interpretability in the Wild\" (Wang et al. 2022)**\n",
    "\n",
    "This notebook validates our implementation of the Indirect Object Identification (IOI) circuit discovery against the original paper's findings.\n",
    "\n",
    "## Paper Reference\n",
    "Wang, K., et al. (2022). Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small. *ICLR 2023*.\n",
    "\n",
    "## Implementation Reference\n",
    "ARENA 1.4.1: Indirect Object Identification\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/environments/mech_interp/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_ioi_dataset' from 'src.data.dataset' (/home/alex/mech_interp/IOI_implementation/src/data/dataset.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m sys.path.append(\u001b[33m'\u001b[39m\u001b[33m..\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Import our modules\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generate_ioi_dataset, load_ioi_dataset\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_ioi_model\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manalysis\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mioi_baseline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_baseline, compute_logit_diff\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'load_ioi_dataset' from 'src.data.dataset' (/home/alex/mech_interp/IOI_implementation/src/data/dataset.py)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import our modules\n",
    "from src.data.dataset import generate_ioi_dataset, load_ioi_dataset\n",
    "from src.model.model_loader import load_ioi_model\n",
    "from src.analysis.ioi_baseline import run_baseline, compute_logit_diff\n",
    "from src.analysis.activation_patching import (\n",
    "    patch_all_layers,\n",
    "    patch_all_heads,\n",
    "    analyze_example_patching\n",
    ")\n",
    "from src.analysis.attention_analysis import (\n",
    "    find_all_ioi_heads,\n",
    "    analyze_duplicate_token_attention,\n",
    "    analyze_s_inhibition_attention,\n",
    "    analyze_name_mover_attention\n",
    ")\n",
    "from src.analysis.path_patching import analyze_ioi_circuit_paths\n",
    "from src.analysis.circuit_discovery import (\n",
    "    discover_ioi_circuit,\n",
    "    validate_circuit,\n",
    "    print_circuit_summary\n",
    ")\n",
    "from src.analysis.logit_attribution import (\n",
    "    compare_io_vs_s_attribution,\n",
    "    plot_logit_attribution,\n",
    "    analyze_circuit_with_dla\n",
    ")\n",
    "\n",
    "# Plotting configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"âœ“ Imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Results from Original Paper\n",
    "\n",
    "### Key Findings to Validate:\n",
    "\n",
    "1. **Baseline Performance**\n",
    "   - Model accuracy on IOI task: ~95% (paper reports high accuracy)\n",
    "   - Logit difference (IO - S): Positive and substantial (typically 5-10)\n",
    "\n",
    "2. **Circuit Components** (from paper Fig 2 & 3)\n",
    "   - **Duplicate Token Heads** (early layers 0-3): L0H1, L2H2, L3H0, etc.\n",
    "   - **S-Inhibition Heads** (middle layers 7-8): L7H3, L7H9, L8H6, L8H10\n",
    "   - **Name Mover Heads** (late layers 9-11): L9H6, L9H9, L10H0, L10H2, L11H10\n",
    "\n",
    "3. **Activation Patching Effects**\n",
    "   - Name mover heads: High positive effect (>0.5)\n",
    "   - S-inhibition heads: Moderate positive effect (0.2-0.5)\n",
    "   - Duplicate token heads: Low-moderate effect (0.1-0.3)\n",
    "\n",
    "4. **Path Patching**\n",
    "   - Strong paths: Duplicate â†’ S-inhibition\n",
    "   - Strong paths: S-inhibition â†’ Name mover\n",
    "   - Some direct paths: Duplicate â†’ Name mover\n",
    "\n",
    "5. **Direct Logit Attribution**\n",
    "   - Circuit heads account for 80-95% of logit difference\n",
    "   - Name movers: Large positive contribution to IO token\n",
    "   - S-inhibition: Large negative contribution to S token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 1: Data Generation and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 1: DATA GENERATION AND MODEL LOADING\n",
      "================================================================================\n",
      "\n",
      "1. Generating ABBA dataset (clean prompts)...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_ioi_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m1. Generating ABBA dataset (clean prompts)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m generate_ioi_dataset(n_examples=\u001b[32m500\u001b[39m, template=\u001b[33m\"\u001b[39m\u001b[33mABBA\u001b[39m\u001b[33m\"\u001b[39m, seed=\u001b[32m42\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m abba_dataset = \u001b[43mload_ioi_dataset\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33m../data/ioi_abba.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Generated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(abba_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ABBA examples\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m2. Generating ABC dataset (corrupted prompts)...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'load_ioi_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PHASE 1: DATA GENERATION AND MODEL LOADING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Generate datasets\n",
    "print(\"\\n1. Generating ABBA dataset (clean prompts)...\")\n",
    "generate_ioi_dataset(n_examples=500, template=\"ABBA\", seed=42)\n",
    "abba_dataset = load_ioi_dataset(\"../data/ioi_abba.json\")\n",
    "print(f\"   Generated {len(abba_dataset)} ABBA examples\")\n",
    "\n",
    "print(\"\\n2. Generating ABC dataset (corrupted prompts)...\")\n",
    "generate_ioi_dataset(n_examples=500, template=\"ABC\", seed=42)\n",
    "abc_dataset = load_ioi_dataset(\"../data/ioi_abc.json\")\n",
    "print(f\"   Generated {len(abc_dataset)} ABC examples\")\n",
    "\n",
    "# Load model\n",
    "print(\"\\n3. Loading GPT-2 Small with TransformerLens...\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "result = load_ioi_model(device=device)\n",
    "model = result[\"model\"]\n",
    "config = result[\"config\"]\n",
    "\n",
    "print(f\"   Model: {config['model_name']}\")\n",
    "print(f\"   Layers: {config['n_layers']}\")\n",
    "print(f\"   Heads: {config['n_heads']}\")\n",
    "print(f\"   Device: {config['device']}\")\n",
    "\n",
    "print(\"\\nâœ“ Phase 1 complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 2: Baseline Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 2: BASELINE PERFORMANCE\n",
      "================================================================================\n",
      "\n",
      "Running baseline analysis on 100 examples...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'run_baseline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Run baseline on sample\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRunning baseline analysis on 100 examples...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m baseline_results = \u001b[43mrun_baseline\u001b[49m(\n\u001b[32m      8\u001b[39m     model,\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m../data/ioi_abba.json\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m     max_examples=\u001b[32m100\u001b[39m\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Extract metrics\u001b[39;00m\n\u001b[32m     14\u001b[39m accuracy = baseline_results[\u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'run_baseline' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PHASE 2: BASELINE PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Run baseline on sample\n",
    "print(\"\\nRunning baseline analysis on 100 examples...\")\n",
    "baseline_results = run_baseline(\n",
    "    model,\n",
    "    \"../data/ioi_abba.json\",\n",
    "    max_examples=100\n",
    ")\n",
    "\n",
    "# Extract metrics\n",
    "accuracy = baseline_results[\"accuracy\"]\n",
    "mean_logit_diff = baseline_results[\"mean_logit_diff\"]\n",
    "std_logit_diff = baseline_results[\"std_logit_diff\"]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASELINE RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy: {accuracy:.1%}\")\n",
    "print(f\"Mean Logit Diff (IO - S): {mean_logit_diff:.3f} Â± {std_logit_diff:.3f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Validation against paper\n",
    "validation_report = []\n",
    "\n",
    "print(\"\\nðŸ“Š VALIDATION CHECK 1: Baseline Performance\")\n",
    "if accuracy >= 0.90:\n",
    "    print(f\"   âœ“ PASS: Accuracy {accuracy:.1%} >= 90% (paper expects ~95%)\")\n",
    "    validation_report.append({\"test\": \"Baseline Accuracy\", \"status\": \"PASS\", \"value\": f\"{accuracy:.1%}\"})\n",
    "else:\n",
    "    print(f\"   âœ— FAIL: Accuracy {accuracy:.1%} < 90%\")\n",
    "    validation_report.append({\"test\": \"Baseline Accuracy\", \"status\": \"FAIL\", \"value\": f\"{accuracy:.1%}\"})\n",
    "\n",
    "if mean_logit_diff >= 3.0:\n",
    "    print(f\"   âœ“ PASS: Mean logit diff {mean_logit_diff:.3f} >= 3.0\")\n",
    "    validation_report.append({\"test\": \"Logit Difference\", \"status\": \"PASS\", \"value\": f\"{mean_logit_diff:.3f}\"})\n",
    "else:\n",
    "    print(f\"   âœ— FAIL: Mean logit diff {mean_logit_diff:.3f} < 3.0\")\n",
    "    validation_report.append({\"test\": \"Logit Difference\", \"status\": \"FAIL\", \"value\": f\"{mean_logit_diff:.3f}\"})\n",
    "\n",
    "print(\"\\nâœ“ Phase 2 complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 3: Activation Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 3: ACTIVATION PATCHING\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'abba_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Use first example for detailed analysis\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m example = \u001b[43mabba_dataset\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m      7\u001b[39m clean_prompt = example[\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mClean prompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclean_prompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'abba_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PHASE 3: ACTIVATION PATCHING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use first example for detailed analysis\n",
    "example = abba_dataset[0]\n",
    "clean_prompt = example[\"prompt\"]\n",
    "print(f\"\\nClean prompt: {clean_prompt}\")\n",
    "\n",
    "# Create corrupted version\n",
    "corrupted_prompt = clean_prompt.replace(\n",
    "    f\", {example['io_name']} gave\",\n",
    "    f\", {example['s_name']} gave\"\n",
    ")\n",
    "print(f\"Corrupted prompt: {corrupted_prompt}\")\n",
    "\n",
    "clean_tokens = model.to_tokens(clean_prompt)\n",
    "corrupted_tokens = model.to_tokens(corrupted_prompt)\n",
    "\n",
    "io_token_id = model.to_single_token(\" \" + example[\"io_name\"])\n",
    "s_token_id = model.to_single_token(\" \" + example[\"s_name\"])\n",
    "\n",
    "# Patch all layers\n",
    "print(\"\\n1. Patching residual stream at all layers...\")\n",
    "layer_effects = patch_all_layers(\n",
    "    model, clean_tokens, corrupted_tokens,\n",
    "    io_token_id, s_token_id\n",
    ")\n",
    "\n",
    "# Plot layer effects\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(range(len(layer_effects)), layer_effects, marker='o')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Patching Effect')\n",
    "plt.title('Activation Patching by Layer\\n(Effect of restoring clean activations)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='High Effect (>0.5)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/activation_patching_layers.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTop 3 most important layers:\")\n",
    "top_layers = sorted(enumerate(layer_effects), key=lambda x: x[1], reverse=True)[:3]\n",
    "for layer, effect in top_layers:\n",
    "    print(f\"   Layer {layer}: {effect:.3f}\")\n",
    "\n",
    "# Patch all heads\n",
    "print(\"\\n2. Patching all attention heads...\")\n",
    "head_effects = patch_all_heads(\n",
    "    model, clean_tokens, corrupted_tokens,\n",
    "    io_token_id, s_token_id\n",
    ")\n",
    "\n",
    "# Reshape for heatmap\n",
    "n_layers = model.cfg.n_layers\n",
    "n_heads = model.cfg.n_heads\n",
    "head_matrix = np.zeros((n_layers, n_heads))\n",
    "for (layer, head), effect in head_effects.items():\n",
    "    head_matrix[layer, head] = effect\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(\n",
    "    head_matrix,\n",
    "    cmap='RdYlGn',\n",
    "    center=0,\n",
    "    vmin=-0.2,\n",
    "    vmax=1.0,\n",
    "    cbar_kws={'label': 'Patching Effect'},\n",
    "    xticklabels=range(n_heads),\n",
    "    yticklabels=range(n_layers)\n",
    ")\n",
    "plt.xlabel('Head')\n",
    "plt.ylabel('Layer')\n",
    "plt.title('Activation Patching by Head\\n(Higher = More Important for IOI Task)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/activation_patching_heads.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Find top heads\n",
    "print(f\"\\nTop 10 most important heads:\")\n",
    "top_heads = sorted(head_effects.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for (layer, head), effect in top_heads:\n",
    "    print(f\"   L{layer}H{head}: {effect:.3f}\")\n",
    "\n",
    "print(\"\\nâœ“ Phase 3 complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 4: Attention Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 4: ATTENTION PATTERN ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Finding IOI circuit heads using attention patterns...\n",
      "(Using 50 examples for robustness)\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'find_all_ioi_heads' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFinding IOI circuit heads using attention patterns...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m(Using 50 examples for robustness)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m ioi_heads = \u001b[43mfind_all_ioi_heads\u001b[49m(\n\u001b[32m     10\u001b[39m     model,\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m../data/ioi_abba.json\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m     max_examples=\u001b[32m50\u001b[39m,\n\u001b[32m     13\u001b[39m     dup_threshold=\u001b[32m0.4\u001b[39m,\n\u001b[32m     14\u001b[39m     s_inhibition_threshold=\u001b[32m0.3\u001b[39m,\n\u001b[32m     15\u001b[39m     name_mover_threshold=\u001b[32m0.3\u001b[39m\n\u001b[32m     16\u001b[39m )\n\u001b[32m     18\u001b[39m duplicate_heads = ioi_heads[\u001b[33m\"\u001b[39m\u001b[33mduplicate_token_heads\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     19\u001b[39m s_inhibition_heads = ioi_heads[\u001b[33m\"\u001b[39m\u001b[33ms_inhibition_heads\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'find_all_ioi_heads' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PHASE 4: ATTENTION PATTERN ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find all IOI heads using attention patterns\n",
    "print(\"\\nFinding IOI circuit heads using attention patterns...\")\n",
    "print(\"(Using 50 examples for robustness)\\n\")\n",
    "\n",
    "ioi_heads = find_all_ioi_heads(\n",
    "    model,\n",
    "    \"../data/ioi_abba.json\",\n",
    "    max_examples=50,\n",
    "    dup_threshold=0.4,\n",
    "    s_inhibition_threshold=0.3,\n",
    "    name_mover_threshold=0.3\n",
    ")\n",
    "\n",
    "duplicate_heads = ioi_heads[\"duplicate_token_heads\"]\n",
    "s_inhibition_heads = ioi_heads[\"s_inhibition_heads\"]\n",
    "name_mover_heads = ioi_heads[\"name_mover_heads\"]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DISCOVERED CIRCUIT HEADS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDuplicate Token Heads ({len(duplicate_heads)}):\")\n",
    "for layer, head in sorted(duplicate_heads):\n",
    "    print(f\"   L{layer}H{head}\")\n",
    "\n",
    "print(f\"\\nS-Inhibition Heads ({len(s_inhibition_heads)}):\")\n",
    "for layer, head in sorted(s_inhibition_heads):\n",
    "    print(f\"   L{layer}H{head}\")\n",
    "\n",
    "print(f\"\\nName Mover Heads ({len(name_mover_heads)}):\")\n",
    "for layer, head in sorted(name_mover_heads):\n",
    "    print(f\"   L{layer}H{head}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Compare with paper's reported heads\n",
    "paper_name_movers = [(9, 6), (9, 9), (10, 0), (10, 2)]\n",
    "paper_s_inhibition = [(7, 3), (7, 9), (8, 6), (8, 10)]\n",
    "\n",
    "print(\"\\nðŸ“Š VALIDATION CHECK 2: Circuit Head Discovery\")\n",
    "\n",
    "# Check name movers\n",
    "nm_overlap = len(set(name_mover_heads) & set(paper_name_movers))\n",
    "print(f\"\\nName Mover Heads:\")\n",
    "print(f\"   Paper reports: {paper_name_movers}\")\n",
    "print(f\"   We found: {sorted(name_mover_heads)}\")\n",
    "print(f\"   Overlap: {nm_overlap}/{len(paper_name_movers)}\")\n",
    "if nm_overlap >= 2:\n",
    "    print(f\"   âœ“ PASS: Found {nm_overlap} of paper's key name movers\")\n",
    "    validation_report.append({\"test\": \"Name Mover Discovery\", \"status\": \"PASS\", \"value\": f\"{nm_overlap}/{len(paper_name_movers)}\"})\n",
    "else:\n",
    "    print(f\"   âœ— FAIL: Only found {nm_overlap} of paper's key name movers\")\n",
    "    validation_report.append({\"test\": \"Name Mover Discovery\", \"status\": \"FAIL\", \"value\": f\"{nm_overlap}/{len(paper_name_movers)}\"})\n",
    "\n",
    "# Check S-inhibition\n",
    "si_overlap = len(set(s_inhibition_heads) & set(paper_s_inhibition))\n",
    "print(f\"\\nS-Inhibition Heads:\")\n",
    "print(f\"   Paper reports: {paper_s_inhibition}\")\n",
    "print(f\"   We found: {sorted(s_inhibition_heads)}\")\n",
    "print(f\"   Overlap: {si_overlap}/{len(paper_s_inhibition)}\")\n",
    "if si_overlap >= 2:\n",
    "    print(f\"   âœ“ PASS: Found {si_overlap} of paper's key S-inhibition heads\")\n",
    "    validation_report.append({\"test\": \"S-Inhibition Discovery\", \"status\": \"PASS\", \"value\": f\"{si_overlap}/{len(paper_s_inhibition)}\"})\n",
    "else:\n",
    "    print(f\"   âœ— FAIL: Only found {si_overlap} of paper's key S-inhibition heads\")\n",
    "    validation_report.append({\"test\": \"S-Inhibition Discovery\", \"status\": \"FAIL\", \"value\": f\"{si_overlap}/{len(paper_s_inhibition)}\"})\n",
    "\n",
    "# Check duplicate token heads are in early layers\n",
    "dup_in_early = sum(1 for layer, head in duplicate_heads if layer <= 3)\n",
    "print(f\"\\nDuplicate Token Heads:\")\n",
    "print(f\"   We found: {sorted(duplicate_heads)}\")\n",
    "print(f\"   In early layers (0-3): {dup_in_early}/{len(duplicate_heads)}\")\n",
    "if dup_in_early >= len(duplicate_heads) * 0.6:\n",
    "    print(f\"   âœ“ PASS: Most duplicate token heads are in early layers\")\n",
    "    validation_report.append({\"test\": \"Duplicate Head Layers\", \"status\": \"PASS\", \"value\": f\"{dup_in_early}/{len(duplicate_heads)}\"})\n",
    "else:\n",
    "    print(f\"   âœ— FAIL: Not enough duplicate token heads in early layers\")\n",
    "    validation_report.append({\"test\": \"Duplicate Head Layers\", \"status\": \"FAIL\", \"value\": f\"{dup_in_early}/{len(duplicate_heads)}\"})\n",
    "\n",
    "print(\"\\nâœ“ Phase 4 complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 5: Path Patching Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 5: PATH PATCHING ANALYSIS\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'duplicate_heads' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPHASE 5: PATH PATCHING ANALYSIS\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mduplicate_heads\u001b[49m) > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(s_inhibition_heads) > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(name_mover_heads) > \u001b[32m0\u001b[39m:\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAnalyzing paths between circuit components...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m     path_results = analyze_ioi_circuit_paths(\n\u001b[32m      9\u001b[39m         model,\n\u001b[32m     10\u001b[39m         clean_tokens,\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m         s_token_id\n\u001b[32m     17\u001b[39m     )\n",
      "\u001b[31mNameError\u001b[39m: name 'duplicate_heads' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PHASE 5: PATH PATCHING ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(duplicate_heads) > 0 and len(s_inhibition_heads) > 0 and len(name_mover_heads) > 0:\n",
    "    print(\"\\nAnalyzing paths between circuit components...\")\n",
    "    \n",
    "    path_results = analyze_ioi_circuit_paths(\n",
    "        model,\n",
    "        clean_tokens,\n",
    "        corrupted_tokens,\n",
    "        duplicate_heads[:3],  # Use top 3 of each type\n",
    "        s_inhibition_heads[:3],\n",
    "        name_mover_heads[:3],\n",
    "        io_token_id,\n",
    "        s_token_id\n",
    "    )\n",
    "    \n",
    "    # Analyze duplicate â†’ S-inhibition paths\n",
    "    print(\"\\n1. Duplicate Token â†’ S-Inhibition Paths:\")\n",
    "    dup_to_si = path_results[\"dup_to_s_inhibition\"][\"effect_matrix\"]\n",
    "    max_dup_si = np.max(dup_to_si)\n",
    "    print(f\"   Max effect: {max_dup_si:.3f}\")\n",
    "    print(f\"   Mean effect: {np.mean(dup_to_si):.3f}\")\n",
    "    \n",
    "    # Analyze S-inhibition â†’ Name mover paths\n",
    "    print(\"\\n2. S-Inhibition â†’ Name Mover Paths:\")\n",
    "    si_to_nm = path_results[\"s_inhibition_to_name_mover\"][\"effect_matrix\"]\n",
    "    max_si_nm = np.max(si_to_nm)\n",
    "    print(f\"   Max effect: {max_si_nm:.3f}\")\n",
    "    print(f\"   Mean effect: {np.mean(si_to_nm):.3f}\")\n",
    "    \n",
    "    # Analyze duplicate â†’ Name mover paths\n",
    "    print(\"\\n3. Duplicate Token â†’ Name Mover Paths:\")\n",
    "    dup_to_nm = path_results[\"dup_to_name_mover\"][\"effect_matrix\"]\n",
    "    max_dup_nm = np.max(dup_to_nm)\n",
    "    print(f\"   Max effect: {max_dup_nm:.3f}\")\n",
    "    print(f\"   Mean effect: {np.mean(dup_to_nm):.3f}\")\n",
    "    \n",
    "    print(\"\\nðŸ“Š VALIDATION CHECK 3: Path Importance\")\n",
    "    if max_si_nm > 0.3:\n",
    "        print(f\"   âœ“ PASS: Found strong S-Inhibition â†’ Name Mover path ({max_si_nm:.3f})\")\n",
    "        validation_report.append({\"test\": \"SIâ†’NM Path\", \"status\": \"PASS\", \"value\": f\"{max_si_nm:.3f}\"})\n",
    "    else:\n",
    "        print(f\"   âœ— FAIL: No strong S-Inhibition â†’ Name Mover path\")\n",
    "        validation_report.append({\"test\": \"SIâ†’NM Path\", \"status\": \"FAIL\", \"value\": f\"{max_si_nm:.3f}\"})\n",
    "else:\n",
    "    print(\"\\nâš  Skipping path patching - insufficient heads discovered\")\n",
    "    validation_report.append({\"test\": \"Path Patching\", \"status\": \"SKIP\", \"value\": \"Insufficient heads\"})\n",
    "\n",
    "print(\"\\nâœ“ Phase 5 complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 6: Complete Circuit Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 6: COMPLETE CIRCUIT DISCOVERY\n",
      "================================================================================\n",
      "\n",
      "Running complete circuit discovery pipeline...\n",
      "(This combines attention patterns + activation patching + path patching)\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'discover_ioi_circuit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRunning complete circuit discovery pipeline...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m(This combines attention patterns + activation patching + path patching)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m discovered_circuit = \u001b[43mdiscover_ioi_circuit\u001b[49m(\n\u001b[32m      9\u001b[39m     model,\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m../data/ioi_abba.json\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m     max_examples=\u001b[32m30\u001b[39m,\n\u001b[32m     12\u001b[39m     head_threshold=\u001b[32m0.35\u001b[39m,\n\u001b[32m     13\u001b[39m     path_threshold=\u001b[32m0.25\u001b[39m\n\u001b[32m     14\u001b[39m )\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Print summary\u001b[39;00m\n\u001b[32m     17\u001b[39m print_circuit_summary(discovered_circuit)\n",
      "\u001b[31mNameError\u001b[39m: name 'discover_ioi_circuit' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PHASE 6: COMPLETE CIRCUIT DISCOVERY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nRunning complete circuit discovery pipeline...\")\n",
    "print(\"(This combines attention patterns + activation patching + path patching)\\n\")\n",
    "\n",
    "discovered_circuit = discover_ioi_circuit(\n",
    "    model,\n",
    "    \"../data/ioi_abba.json\",\n",
    "    max_examples=30,\n",
    "    head_threshold=0.35,\n",
    "    path_threshold=0.25\n",
    ")\n",
    "\n",
    "# Print summary\n",
    "print_circuit_summary(discovered_circuit)\n",
    "\n",
    "# Save circuit\n",
    "circuit_path = \"../results/discovered_ioi_circuit.json\"\n",
    "with open(circuit_path, 'w') as f:\n",
    "    # Convert tuples to lists for JSON serialization\n",
    "    json_circuit = {\n",
    "        \"duplicate_token_heads\": [list(h) for h in discovered_circuit[\"duplicate_token_heads\"]],\n",
    "        \"s_inhibition_heads\": [list(h) for h in discovered_circuit[\"s_inhibition_heads\"]],\n",
    "        \"name_mover_heads\": [list(h) for h in discovered_circuit[\"name_mover_heads\"]],\n",
    "        \"critical_paths\": [\n",
    "            {**p, \"from\": list(p[\"from\"]), \"to\": list(p[\"to\"])}\n",
    "            for p in discovered_circuit[\"critical_paths\"]\n",
    "        ],\n",
    "        \"head_effects\": discovered_circuit[\"head_effects\"],\n",
    "        \"metadata\": discovered_circuit[\"metadata\"]\n",
    "    }\n",
    "    json.dump(json_circuit, f, indent=2)\n",
    "print(f\"\\nðŸ’¾ Saved circuit to {circuit_path}\")\n",
    "\n",
    "# Validate circuit\n",
    "print(\"\\nValidating discovered circuit...\")\n",
    "validation_results = validate_circuit(\n",
    "    model,\n",
    "    \"../data/ioi_abba.json\",\n",
    "    discovered_circuit,\n",
    "    max_examples=50\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Phase 6 complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 7: Direct Logit Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 7: DIRECT LOGIT ATTRIBUTION\n",
      "================================================================================\n",
      "\n",
      "Computing direct logit attribution for IO vs S tokens...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clean_prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mComputing direct logit attribution for IO vs S tokens...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Use the same example from before\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPrompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mclean_prompt\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIO token: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample[\u001b[33m'\u001b[39m\u001b[33mio_name\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mS token: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample[\u001b[33m'\u001b[39m\u001b[33ms_name\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'clean_prompt' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PHASE 7: DIRECT LOGIT ATTRIBUTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nComputing direct logit attribution for IO vs S tokens...\")\n",
    "\n",
    "# Use the same example from before\n",
    "print(f\"\\nPrompt: {clean_prompt}\")\n",
    "print(f\"IO token: {example['io_name']}\")\n",
    "print(f\"S token: {example['s_name']}\")\n",
    "\n",
    "# Compare IO vs S attribution\n",
    "comparison = compare_io_vs_s_attribution(\n",
    "    model,\n",
    "    clean_tokens,\n",
    "    io_token_id,\n",
    "    s_token_id\n",
    ")\n",
    "\n",
    "print(f\"\\nLogit difference (IO - S): {comparison['logit_diff']:.3f}\")\n",
    "\n",
    "# Show top contributors\n",
    "print(\"\\nTop 5 heads contributing to IO token:\")\n",
    "for layer, head, contrib in comparison[\"top_io_heads\"][:5]:\n",
    "    print(f\"   L{layer}H{head}: {contrib:6.3f}\")\n",
    "\n",
    "print(\"\\nTop 5 heads suppressing S token:\")\n",
    "for layer, head, contrib in comparison[\"top_s_suppression_heads\"][:5]:\n",
    "    print(f\"   L{layer}H{head}: {contrib:6.3f}\")\n",
    "\n",
    "# Create visualization\n",
    "plot_logit_attribution(\n",
    "    comparison,\n",
    "    save_path=\"../results/logit_attribution.png\",\n",
    "    top_n=15\n",
    ")\n",
    "\n",
    "# Analyze circuit with DLA\n",
    "print(\"\\nAnalyzing circuit with Direct Logit Attribution...\")\n",
    "dla_analysis = analyze_circuit_with_dla(\n",
    "    model,\n",
    "    clean_tokens,\n",
    "    io_token_id,\n",
    "    s_token_id,\n",
    "    circuit_heads=discovered_circuit\n",
    ")\n",
    "\n",
    "circuit_percentage = dla_analysis[\"circuit_analysis\"][\"circuit_percentage\"]\n",
    "\n",
    "print(\"\\nðŸ“Š VALIDATION CHECK 4: Circuit Attribution\")\n",
    "if circuit_percentage >= 0.60:\n",
    "    print(f\"   âœ“ PASS: Circuit accounts for {circuit_percentage:.1%} of logit diff (â‰¥60%)\")\n",
    "    validation_report.append({\"test\": \"Circuit Attribution\", \"status\": \"PASS\", \"value\": f\"{circuit_percentage:.1%}\"})\n",
    "else:\n",
    "    print(f\"   âœ— FAIL: Circuit only accounts for {circuit_percentage:.1%} of logit diff\")\n",
    "    validation_report.append({\"test\": \"Circuit Attribution\", \"status\": \"FAIL\", \"value\": f\"{circuit_percentage:.1%}\"})\n",
    "\n",
    "print(\"\\nâœ“ Phase 7 complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Validation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINAL VALIDATION REPORT\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'validation_report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Create report dataframe\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m report_df = pd.DataFrame(\u001b[43mvalidation_report\u001b[49m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Display report\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'validation_report' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FINAL VALIDATION REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create report dataframe\n",
    "report_df = pd.DataFrame(validation_report)\n",
    "\n",
    "# Display report\n",
    "print(\"\\n\")\n",
    "print(report_df.to_string(index=False))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Count results\n",
    "n_pass = sum(1 for r in validation_report if r[\"status\"] == \"PASS\")\n",
    "n_fail = sum(1 for r in validation_report if r[\"status\"] == \"FAIL\")\n",
    "n_skip = sum(1 for r in validation_report if r[\"status\"] == \"SKIP\")\n",
    "n_total = len(validation_report)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"RESULTS: {n_pass} PASS | {n_fail} FAIL | {n_skip} SKIP | {n_total} TOTAL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Overall assessment\n",
    "pass_rate = n_pass / max(n_total - n_skip, 1)\n",
    "\n",
    "print(\"\\nðŸ“Š OVERALL ASSESSMENT:\\n\")\n",
    "if pass_rate >= 0.8:\n",
    "    print(\"âœ… EXCELLENT: Our implementation successfully replicates the IOI circuit!\")\n",
    "    print(\"   The discovered circuit matches the paper's key findings.\")\n",
    "elif pass_rate >= 0.6:\n",
    "    print(\"âœ“ GOOD: Our implementation largely replicates the IOI circuit.\")\n",
    "    print(\"   Some minor discrepancies exist but overall structure is correct.\")\n",
    "elif pass_rate >= 0.4:\n",
    "    print(\"âš  PARTIAL: Our implementation partially replicates the IOI circuit.\")\n",
    "    print(\"   Significant discrepancies exist. Further investigation needed.\")\n",
    "else:\n",
    "    print(\"âŒ POOR: Our implementation does not match the paper's findings.\")\n",
    "    print(\"   Major issues detected. Review implementation carefully.\")\n",
    "\n",
    "# Save report\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "report_path = f\"../results/validation_report_{timestamp}.csv\"\n",
    "report_df.to_csv(report_path, index=False)\n",
    "print(f\"\\nðŸ’¾ Saved detailed report to {report_path}\")\n",
    "\n",
    "# Create summary document\n",
    "summary_path = f\"../results/validation_summary_{timestamp}.txt\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"IOI CIRCUIT REPLICATION VALIDATION SUMMARY\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    f.write(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Model: GPT-2 Small\\n\")\n",
    "    f.write(f\"Device: {device}\\n\\n\")\n",
    "    \n",
    "    f.write(\"BASELINE PERFORMANCE\\n\")\n",
    "    f.write(\"-\" * 40 + \"\\n\")\n",
    "    f.write(f\"Accuracy: {accuracy:.1%}\\n\")\n",
    "    f.write(f\"Mean Logit Diff: {mean_logit_diff:.3f} Â± {std_logit_diff:.3f}\\n\\n\")\n",
    "    \n",
    "    f.write(\"DISCOVERED CIRCUIT\\n\")\n",
    "    f.write(\"-\" * 40 + \"\\n\")\n",
    "    f.write(f\"Duplicate Token Heads: {len(discovered_circuit['duplicate_token_heads'])}\\n\")\n",
    "    f.write(f\"S-Inhibition Heads: {len(discovered_circuit['s_inhibition_heads'])}\\n\")\n",
    "    f.write(f\"Name Mover Heads: {len(discovered_circuit['name_mover_heads'])}\\n\")\n",
    "    f.write(f\"Critical Paths: {len(discovered_circuit['critical_paths'])}\\n\\n\")\n",
    "    \n",
    "    f.write(\"VALIDATION RESULTS\\n\")\n",
    "    f.write(\"-\" * 40 + \"\\n\")\n",
    "    f.write(report_df.to_string(index=False))\n",
    "    f.write(\"\\n\\n\")\n",
    "    \n",
    "    f.write(f\"SUMMARY: {n_pass} PASS | {n_fail} FAIL | {n_skip} SKIP\\n\")\n",
    "    f.write(f\"Pass Rate: {pass_rate:.1%}\\n\")\n",
    "\n",
    "print(f\"ðŸ’¾ Saved summary to {summary_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ“ VALIDATION COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "KEY FINDINGS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "1. BASELINE PERFORMANCE\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m1. BASELINE PERFORMANCE\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   - Model accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43maccuracy\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   - Mean logit difference: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_logit_diff\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   - Paper expectation: ~95% accuracy, positive logit diff\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"KEY FINDINGS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. BASELINE PERFORMANCE\")\n",
    "print(f\"   - Model accuracy: {accuracy:.1%}\")\n",
    "print(f\"   - Mean logit difference: {mean_logit_diff:.3f}\")\n",
    "print(f\"   - Paper expectation: ~95% accuracy, positive logit diff\")\n",
    "\n",
    "print(\"\\n2. CIRCUIT COMPONENTS\")\n",
    "print(f\"   - Duplicate Token Heads: {sorted(discovered_circuit['duplicate_token_heads'])}\")\n",
    "print(f\"   - S-Inhibition Heads: {sorted(discovered_circuit['s_inhibition_heads'])}\")\n",
    "print(f\"   - Name Mover Heads: {sorted(discovered_circuit['name_mover_heads'])}\")\n",
    "\n",
    "print(\"\\n3. CIRCUIT VALIDATION\")\n",
    "if \"circuit_analysis\" in dla_analysis:\n",
    "    print(f\"   - Circuit logit diff contribution: {dla_analysis['circuit_analysis']['circuit_logit_diff']:.3f}\")\n",
    "    print(f\"   - Total logit diff: {dla_analysis['circuit_analysis']['total_logit_diff']:.3f}\")\n",
    "    print(f\"   - Circuit percentage: {dla_analysis['circuit_analysis']['circuit_percentage']:.1%}\")\n",
    "    print(f\"   - Paper expectation: 80-95% from circuit heads\")\n",
    "\n",
    "print(\"\\n4. COMPARISON WITH PAPER\")\n",
    "print(f\"   - Name mover overlap: {nm_overlap}/{len(paper_name_movers)} key heads\")\n",
    "print(f\"   - S-inhibition overlap: {si_overlap}/{len(paper_s_inhibition)} key heads\")\n",
    "print(f\"   - Overall validation: {n_pass}/{n_total - n_skip} checks passed\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Notes and Observations\n",
    "\n",
    "### Expected Behavior:\n",
    "1. The model should achieve >90% accuracy on IOI task\n",
    "2. Circuit heads should be concentrated in expected layers:\n",
    "   - Duplicate token heads: Early layers (0-3)\n",
    "   - S-inhibition heads: Middle layers (7-8)\n",
    "   - Name mover heads: Late layers (9-11)\n",
    "3. Circuit heads should account for majority (>60%) of logit difference\n",
    "\n",
    "### Potential Discrepancies:\n",
    "- Exact heads may vary slightly depending on dataset and thresholds\n",
    "- Some heads may have borderline effects and be included/excluded based on threshold\n",
    "- Path effects can be noisy on single examples\n",
    "\n",
    "### Next Steps:\n",
    "1. If validation fails: Try different thresholds or more examples\n",
    "2. Analyze specific discrepancies in detail\n",
    "3. Consider running on additional dataset templates\n",
    "4. Compare attention patterns qualitatively\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0380cf73",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_ioi_dataset' from 'src.data.dataset' (/home/alex/mech_interp/IOI_implementation/src/data/dataset.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m sys.path.append(\u001b[33m'\u001b[39m\u001b[33m..\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Import our modules\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generate_ioi_dataset, load_ioi_dataset\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_ioi_model\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manalysis\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mioi_baseline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_baseline, compute_logit_diff\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'load_ioi_dataset' from 'src.data.dataset' (/home/alex/mech_interp/IOI_implementation/src/data/dataset.py)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import our modules\n",
    "from src.data.dataset import generate_ioi_dataset, load_ioi_dataset\n",
    "from src.model.model_loader import load_ioi_model\n",
    "from src.analysis.ioi_baseline import run_baseline, compute_logit_diff\n",
    "from src.analysis.activation_patching import (\n",
    "    patch_all_layers,\n",
    "    patch_all_heads,\n",
    "    analyze_example_patching\n",
    ")\n",
    "from src.analysis.attention_analysis import (\n",
    "    find_all_ioi_heads,\n",
    "    analyze_duplicate_token_attention,\n",
    "    analyze_s_inhibition_attention,\n",
    "    analyze_name_mover_attention\n",
    ")\n",
    "from src.analysis.path_patching import analyze_ioi_circuit_paths\n",
    "from src.analysis.circuit_discovery import (\n",
    "    discover_ioi_circuit,\n",
    "    validate_circuit,\n",
    "    print_circuit_summary\n",
    ")\n",
    "from src.analysis.logit_attribution import (\n",
    "    compare_io_vs_s_attribution,\n",
    "    plot_logit_attribution,\n",
    "    analyze_circuit_with_dla\n",
    ")\n",
    "\n",
    "# Plotting configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ“ Imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2cdf810",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_ioi_dataset' from 'src.data.dataset' (/home/alex/mech_interp/IOI_implementation/src/data/dataset.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m sys.path.append(\u001b[33m'\u001b[39m\u001b[33m..\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Import our modules\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generate_ioi_dataset, load_ioi_dataset\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_ioi_model\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manalysis\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mioi_baseline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_baseline, compute_logit_diff\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'load_ioi_dataset' from 'src.data.dataset' (/home/alex/mech_interp/IOI_implementation/src/data/dataset.py)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import our modules\n",
    "from src.data.dataset import generate_ioi_dataset, load_ioi_dataset\n",
    "from src.model.model_loader import load_ioi_model\n",
    "from src.analysis.ioi_baseline import run_baseline, compute_logit_diff\n",
    "from src.analysis.activation_patching import (\n",
    "    patch_all_layers,\n",
    "    patch_all_heads,\n",
    "    analyze_example_patching\n",
    ")\n",
    "from src.analysis.attention_analysis import (\n",
    "    find_all_ioi_heads,\n",
    "    analyze_duplicate_token_attention,\n",
    "    analyze_s_inhibition_attention,\n",
    "    analyze_name_mover_attention\n",
    ")\n",
    "from src.analysis.path_patching import analyze_ioi_circuit_paths\n",
    "from src.analysis.circuit_discovery import (\n",
    "    discover_ioi_circuit,\n",
    "    validate_circuit,\n",
    "    print_circuit_summary\n",
    ")\n",
    "from src.analysis.logit_attribution import (\n",
    "    compare_io_vs_s_attribution,\n",
    "    plot_logit_attribution,\n",
    "    analyze_circuit_with_dla\n",
    ")\n",
    "\n",
    "# Plotting configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ“ Imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d461a42e",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Dataset module loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Force reload the dataset module\n",
    "import src.data.dataset\n",
    "importlib.reload(src.data.dataset)\n",
    "\n",
    "# Now import everything\n",
    "from src.data.dataset import generate_ioi_dataset, load_ioi_dataset\n",
    "print(\"âœ“ Dataset module loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81bfd8d0",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All imports successful\n",
      "PyTorch version: 2.9.0+cu128\n",
      "Device: CPU\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "# Import remaining modules\n",
    "from src.model.model_loader import load_ioi_model\n",
    "from src.analysis.ioi_baseline import run_baseline, compute_logit_diff\n",
    "from src.analysis.activation_patching import (\n",
    "    patch_all_layers,\n",
    "    patch_all_heads,\n",
    "    analyze_example_patching\n",
    ")\n",
    "from src.analysis.attention_analysis import (\n",
    "    find_all_ioi_heads,\n",
    "    analyze_duplicate_token_attention,\n",
    "    analyze_s_inhibition_attention,\n",
    "    analyze_name_mover_attention\n",
    ")\n",
    "from src.analysis.path_patching import analyze_ioi_circuit_paths\n",
    "from src.analysis.circuit_discovery import (\n",
    "    discover_ioi_circuit,\n",
    "    validate_circuit,\n",
    "    print_circuit_summary\n",
    ")\n",
    "from src.analysis.logit_attribution import (\n",
    "    compare_io_vs_s_attribution,\n",
    "    plot_logit_attribution,\n",
    "    analyze_circuit_with_dla\n",
    ")\n",
    "\n",
    "# Plotting configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ“ All imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a66d237f",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 1: DATA GENERATION AND MODEL LOADING\n",
      "================================================================================\n",
      "\n",
      "1. Generating ABBA dataset (clean prompts)...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/ioi_abba.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m1. Generating ABBA dataset (clean prompts)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m generate_ioi_dataset(n_examples=\u001b[32m500\u001b[39m, template=\u001b[33m\"\u001b[39m\u001b[33mABBA\u001b[39m\u001b[33m\"\u001b[39m, seed=\u001b[32m42\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m abba_dataset = \u001b[43mload_ioi_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../data/ioi_abba.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Generated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(abba_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ABBA examples\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m2. Generating ABC dataset (corrupted prompts)...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mech_interp/IOI_implementation/src/data/dataset.py:210\u001b[39m, in \u001b[36mload_ioi_dataset\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_ioi_dataset\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) -> List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    201\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[33;03m    Load IOI dataset from JSON file.\u001b[39;00m\n\u001b[32m    203\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    208\u001b[39m \u001b[33;03m        List of dataset examples\u001b[39;00m\n\u001b[32m    209\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    211\u001b[39m         dataset = json.load(f)\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../data/ioi_abba.json'"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PHASE 1: DATA GENERATION AND MODEL LOADING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Generate datasets\n",
    "print(\"\\n1. Generating ABBA dataset (clean prompts)...\")\n",
    "generate_ioi_dataset(n_examples=500, template=\"ABBA\", seed=42)\n",
    "abba_dataset = load_ioi_dataset(\"../data/ioi_abba.json\")\n",
    "print(f\"   Generated {len(abba_dataset)} ABBA examples\")\n",
    "\n",
    "print(\"\\n2. Generating ABC dataset (corrupted prompts)...\")\n",
    "generate_ioi_dataset(n_examples=500, template=\"ABC\", seed=42)\n",
    "abc_dataset = load_ioi_dataset(\"../data/ioi_abc.json\")\n",
    "print(f\"   Generated {len(abc_dataset)} ABC examples\")\n",
    "\n",
    "# Load model\n",
    "print(\"\\n3. Loading GPT-2 Small with TransformerLens...\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "result = load_ioi_model(device=device)\n",
    "model = result[\"model\"]\n",
    "config = result[\"config\"]\n",
    "\n",
    "print(f\"   Model: {config['model_name']}\")\n",
    "print(f\"   Layers: {config['n_layers']}\")\n",
    "print(f\"   Heads: {config['n_heads']}\")\n",
    "print(f\"   Device: {config['device']}\")\n",
    "\n",
    "print(\"\\nâœ“ Phase 1 complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bec17ff1",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 1: DATA GENERATION AND MODEL LOADING\n",
      "================================================================================\n",
      "\n",
      "1. Generating ABBA dataset (clean prompts)...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/ioi_abba.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m1. Generating ABBA dataset (clean prompts)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m generate_ioi_dataset(n_examples=\u001b[32m500\u001b[39m, template=\u001b[33m\"\u001b[39m\u001b[33mABBA\u001b[39m\u001b[33m\"\u001b[39m, seed=\u001b[32m42\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m abba_dataset = \u001b[43mload_ioi_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../data/ioi_abba.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Generated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(abba_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ABBA examples\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m2. Generating ABC dataset (corrupted prompts)...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mech_interp/IOI_implementation/src/data/dataset.py:211\u001b[39m, in \u001b[36mload_ioi_dataset\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_ioi_dataset\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) -> List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    202\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[33;03m    Load IOI dataset from JSON file.\u001b[39;00m\n\u001b[32m    204\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    209\u001b[39m \u001b[33;03m        List of dataset examples\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    212\u001b[39m         dataset = json.load(f)\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../data/ioi_abba.json'"
     ]
    }
   ],
   "source": [
    "# Reload the dataset module to pick up the fix\n",
    "importlib.reload(src.data.dataset)\n",
    "from src.data.dataset import generate_ioi_dataset, load_ioi_dataset\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 1: DATA GENERATION AND MODEL LOADING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Generate datasets\n",
    "print(\"\\n1. Generating ABBA dataset (clean prompts)...\")\n",
    "generate_ioi_dataset(n_examples=500, template=\"ABBA\", seed=42)\n",
    "abba_dataset = load_ioi_dataset(\"../data/ioi_abba.json\")\n",
    "print(f\"   Generated {len(abba_dataset)} ABBA examples\")\n",
    "\n",
    "print(\"\\n2. Generating ABC dataset (corrupted prompts)...\")\n",
    "generate_ioi_dataset(n_examples=500, template=\"ABC\", seed=42)\n",
    "abc_dataset = load_ioi_dataset(\"../data/ioi_abc.json\")\n",
    "print(f\"   Generated {len(abc_dataset)} ABC examples\")\n",
    "\n",
    "# Load model\n",
    "print(\"\\n3. Loading GPT-2 Small with TransformerLens...\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "result = load_ioi_model(device=device)\n",
    "model = result[\"model\"]\n",
    "config = result[\"config\"]\n",
    "\n",
    "print(f\"   Model: {config['model_name']}\")\n",
    "print(f\"   Layers: {config['n_layers']}\")\n",
    "print(f\"   Heads: {config['n_heads']}\")\n",
    "print(f\"   Device: {config['device']}\")\n",
    "\n",
    "print(\"\\nâœ“ Phase 1 complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fed90937",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/alex/mech_interp/IOI_implementation\n",
      "Checking if file exists: False\n",
      "Checking absolute path: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Checking if file exists: {os.path.exists('../data/ioi_abba.json')}\")\n",
    "print(f\"Checking absolute path: {os.path.exists('/home/alex/mech_interp/IOI_implementation/data/ioi_abba.json')}\")\n",
    "\n",
    "# List files in data directory\n",
    "if os.path.exists('../data'):\n",
    "    print(f\"\\nFiles in ../data/:\")\n",
    "    for f in os.listdir('../data'):\n",
    "        print(f\"  {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "710ccd58",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 1: DATA GENERATION AND MODEL LOADING\n",
      "================================================================================\n",
      "\n",
      "1. Generating ABBA dataset (clean prompts)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Generated 500 ABBA examples\n",
      "\n",
      "2. Generating ABC dataset (corrupted prompts)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Generated 500 ABC examples\n",
      "\n",
      "3. Loading GPT-2 Small with TransformerLens...\n",
      "Loading GPT2-small on cpu...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Model loaded successfully!\n",
      "  Layers: 12\n",
      "  Heads: 12\n",
      "  Model dim: 768\n",
      "  Vocab size: 50257\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'model_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m model = result[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     21\u001b[39m config = result[\u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel_name\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Layers: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[33m'\u001b[39m\u001b[33mn_layers\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Heads: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[33m'\u001b[39m\u001b[33mn_heads\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'model_name'"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PHASE 1: DATA GENERATION AND MODEL LOADING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Generate datasets\n",
    "print(\"\\n1. Generating ABBA dataset (clean prompts)...\")\n",
    "generate_ioi_dataset(n_examples=500, template=\"ABBA\", seed=42)\n",
    "abba_dataset = load_ioi_dataset(\"data/ioi_abba.json\")\n",
    "print(f\"   Generated {len(abba_dataset)} ABBA examples\")\n",
    "\n",
    "print(\"\\n2. Generating ABC dataset (corrupted prompts)...\")\n",
    "generate_ioi_dataset(n_examples=500, template=\"ABC\", seed=42)\n",
    "abc_dataset = load_ioi_dataset(\"data/ioi_abc.json\")\n",
    "print(f\"   Generated {len(abc_dataset)} ABC examples\")\n",
    "\n",
    "# Load model\n",
    "print(\"\\n3. Loading GPT-2 Small with TransformerLens...\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "result = load_ioi_model(device=device)\n",
    "model = result[\"model\"]\n",
    "config = result[\"config\"]\n",
    "\n",
    "print(f\"   Model: {config['model_name']}\")\n",
    "print(f\"   Layers: {config['n_layers']}\")\n",
    "print(f\"   Heads: {config['n_heads']}\")\n",
    "print(f\"   Device: {config['device']}\")\n",
    "\n",
    "print(\"\\nâœ“ Phase 1 complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9387b7f5",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config keys: dict_keys(['n_layers', 'n_heads', 'd_model', 'd_vocab', 'd_head', 'n_ctx', 'device'])\n",
      "\n",
      "Config contents:\n",
      "  n_layers: 12\n",
      "  n_heads: 12\n",
      "  d_model: 768\n",
      "  d_vocab: 50257\n",
      "  d_head: 64\n",
      "  n_ctx: 1024\n",
      "  device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check what keys are in the config\n",
    "print(\"Config keys:\", config.keys())\n",
    "print(\"\\nConfig contents:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a243a368",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Model: gpt2-small\n",
      "   Layers: 12\n",
      "   Heads: 12\n",
      "   Device: cpu\n",
      "\n",
      "âœ“ Phase 1 complete\n"
     ]
    }
   ],
   "source": [
    "print(f\"   Model: gpt2-small\")\n",
    "print(f\"   Layers: {config['n_layers']}\")\n",
    "print(f\"   Heads: {config['n_heads']}\")\n",
    "print(f\"   Device: {config['device']}\")\n",
    "\n",
    "print(\"\\nâœ“ Phase 1 complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dae24368",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 2: BASELINE PERFORMANCE\n",
      "================================================================================\n",
      "\n",
      "Running baseline analysis on 100 examples...\n",
      "Loading dataset from HookedTransformer(\n",
      "  (embed): Embed()\n",
      "  (hook_embed): HookPoint()\n",
      "  (pos_embed): PosEmbed()\n",
      "  (hook_pos_embed): HookPoint()\n",
      "  (blocks): ModuleList(\n",
      "    (0-11): 12 x TransformerBlock(\n",
      "      (ln1): LayerNorm(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (ln2): LayerNorm(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (hook_k): HookPoint()\n",
      "        (hook_q): HookPoint()\n",
      "        (hook_v): HookPoint()\n",
      "        (hook_z): HookPoint()\n",
      "        (hook_attn_scores): HookPoint()\n",
      "        (hook_pattern): HookPoint()\n",
      "        (hook_result): HookPoint()\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (hook_pre): HookPoint()\n",
      "        (hook_post): HookPoint()\n",
      "      )\n",
      "      (hook_attn_in): HookPoint()\n",
      "      (hook_q_input): HookPoint()\n",
      "      (hook_k_input): HookPoint()\n",
      "      (hook_v_input): HookPoint()\n",
      "      (hook_mlp_in): HookPoint()\n",
      "      (hook_attn_out): HookPoint()\n",
      "      (hook_mlp_out): HookPoint()\n",
      "      (hook_resid_pre): HookPoint()\n",
      "      (hook_resid_mid): HookPoint()\n",
      "      (hook_resid_post): HookPoint()\n",
      "    )\n",
      "  )\n",
      "  (ln_final): LayerNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (unembed): Unembed()\n",
      ")...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not HookedTransformer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Run baseline on sample\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRunning baseline analysis on 100 examples...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m baseline_results = \u001b[43mrun_baseline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/ioi_abba.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_examples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Extract metrics\u001b[39;00m\n\u001b[32m     14\u001b[39m accuracy = baseline_results[\u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mech_interp/IOI_implementation/src/analysis/ioi_baseline.py:184\u001b[39m, in \u001b[36mrun_baseline\u001b[39m\u001b[34m(dataset_path, model_name, device, max_examples)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading dataset from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    185\u001b[39m     dataset = json.load(f)\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_examples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mTypeError\u001b[39m: expected str, bytes or os.PathLike object, not HookedTransformer"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PHASE 2: BASELINE PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Run baseline on sample\n",
    "print(\"\\nRunning baseline analysis on 100 examples...\")\n",
    "baseline_results = run_baseline(\n",
    "    model,\n",
    "    \"data/ioi_abba.json\",\n",
    "    max_examples=100\n",
    ")\n",
    "\n",
    "# Extract metrics\n",
    "accuracy = baseline_results[\"accuracy\"]\n",
    "mean_logit_diff = baseline_results[\"mean_logit_diff\"]\n",
    "std_logit_diff = baseline_results[\"std_logit_diff\"]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASELINE RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy: {accuracy:.1%}\")\n",
    "print(f\"Mean Logit Diff (IO - S): {mean_logit_diff:.3f} Â± {std_logit_diff:.3f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Validation against paper\n",
    "validation_report = []\n",
    "\n",
    "print(\"\\nðŸ“Š VALIDATION CHECK 1: Baseline Performance\")\n",
    "if accuracy >= 0.90:\n",
    "    print(f\"   âœ“ PASS: Accuracy {accuracy:.1%} >= 90% (paper expects ~95%)\")\n",
    "    validation_report.append({\"test\": \"Baseline Accuracy\", \"status\": \"PASS\", \"value\": f\"{accuracy:.1%}\"})\n",
    "else:\n",
    "    print(f\"   âœ— FAIL: Accuracy {accuracy:.1%} < 90%\")\n",
    "    validation_report.append({\"test\": \"Baseline Accuracy\", \"status\": \"FAIL\", \"value\": f\"{accuracy:.1%}\"})\n",
    "\n",
    "if mean_logit_diff >= 3.0:\n",
    "    print(f\"   âœ“ PASS: Mean logit diff {mean_logit_diff:.3f} >= 3.0\")\n",
    "    validation_report.append({\"test\": \"Logit Difference\", \"status\": \"PASS\", \"value\": f\"{mean_logit_diff:.3f}\"})\n",
    "else:\n",
    "    print(f\"   âœ— FAIL: Mean logit diff {mean_logit_diff:.3f} < 3.0\")\n",
    "    validation_report.append({\"test\": \"Logit Difference\", \"status\": \"FAIL\", \"value\": f\"{mean_logit_diff:.3f}\"})\n",
    "\n",
    "print(\"\\nâœ“ Phase 2 complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b1c7bc5",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 2: BASELINE PERFORMANCE\n",
      "================================================================================\n",
      "\n",
      "Running baseline analysis on 100 examples...\n",
      "Loading dataset from data/ioi_abba.json...\n",
      "Loaded 100 examples\n",
      "\n",
      "Running baseline evaluation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing example 50/100...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing example 100/100...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BASELINE RESULTS\n",
      "============================================================\n",
      "Number of examples: 100\n",
      "Accuracy: 0.00% (0/100)\n",
      "Mean logit diff: 3.805\n",
      "Median logit diff: 3.844\n",
      "Std logit diff: 1.835\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "BASELINE RESULTS\n",
      "============================================================\n",
      "Accuracy: 0.0%\n",
      "Mean Logit Diff (IO - S): 3.805 Â± 1.835\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š VALIDATION CHECK 1: Baseline Performance\n",
      "   âœ— FAIL: Accuracy 0.0% < 90%\n",
      "   âœ“ PASS: Mean logit diff 3.805 >= 3.0\n",
      "\n",
      "âœ“ Phase 2 complete\n"
     ]
    }
   ],
   "source": [
    "# Reload the module\n",
    "import src.analysis.ioi_baseline\n",
    "importlib.reload(src.analysis.ioi_baseline)\n",
    "from src.analysis.ioi_baseline import run_baseline, compute_logit_diff\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 2: BASELINE PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Run baseline on sample\n",
    "print(\"\\nRunning baseline analysis on 100 examples...\")\n",
    "baseline_results = run_baseline(\n",
    "    model,\n",
    "    \"data/ioi_abba.json\",\n",
    "    max_examples=100\n",
    ")\n",
    "\n",
    "# Extract metrics\n",
    "accuracy = baseline_results[\"accuracy\"]\n",
    "mean_logit_diff = baseline_results[\"mean_logit_diff\"]\n",
    "std_logit_diff = baseline_results[\"std_logit_diff\"]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASELINE RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy: {accuracy:.1%}\")\n",
    "print(f\"Mean Logit Diff (IO - S): {mean_logit_diff:.3f} Â± {std_logit_diff:.3f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Validation against paper\n",
    "validation_report = []\n",
    "\n",
    "print(\"\\nðŸ“Š VALIDATION CHECK 1: Baseline Performance\")\n",
    "if accuracy >= 0.90:\n",
    "    print(f\"   âœ“ PASS: Accuracy {accuracy:.1%} >= 90% (paper expects ~95%)\")\n",
    "    validation_report.append({\"test\": \"Baseline Accuracy\", \"status\": \"PASS\", \"value\": f\"{accuracy:.1%}\"})\n",
    "else:\n",
    "    print(f\"   âœ— FAIL: Accuracy {accuracy:.1%} < 90%\")\n",
    "    validation_report.append({\"test\": \"Baseline Accuracy\", \"status\": \"FAIL\", \"value\": f\"{accuracy:.1%}\"})\n",
    "\n",
    "if mean_logit_diff >= 3.0:\n",
    "    print(f\"   âœ“ PASS: Mean logit diff {mean_logit_diff:.3f} >= 3.0\")\n",
    "    validation_report.append({\"test\": \"Logit Difference\", \"status\": \"PASS\", \"value\": f\"{mean_logit_diff:.3f}\"})\n",
    "else:\n",
    "    print(f\"   âœ— FAIL: Mean logit diff {mean_logit_diff:.3f} < 3.0\")\n",
    "    validation_report.append({\"test\": \"Logit Difference\", \"status\": \"FAIL\", \"value\": f\"{mean_logit_diff:.3f}\"})\n",
    "\n",
    "print(\"\\nâœ“ Phase 2 complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c94006a1",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 2: BASELINE PERFORMANCE\n",
      "================================================================================\n",
      "\n",
      "Running baseline analysis on 100 examples...\n",
      "Loading dataset from data/ioi_abba.json...\n",
      "Loaded 100 examples\n",
      "\n",
      "Running baseline evaluation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing example 50/100...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing example 100/100...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BASELINE RESULTS\n",
      "============================================================\n",
      "Number of examples: 100\n",
      "Accuracy: 87.00% (87/100)\n",
      "Mean logit diff: 4.036\n",
      "Median logit diff: 3.891\n",
      "Std logit diff: 1.633\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "BASELINE RESULTS\n",
      "============================================================\n",
      "Accuracy: 87.0%\n",
      "Mean Logit Diff (IO - S): 4.036 Â± 1.633\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š VALIDATION CHECK 1: Baseline Performance\n",
      "   âœ— FAIL: Accuracy 87.0% < 90%\n",
      "   âœ“ PASS: Mean logit diff 4.036 >= 3.0\n",
      "\n",
      "âœ“ Phase 2 complete\n"
     ]
    }
   ],
   "source": [
    "# Reload the module\n",
    "importlib.reload(src.analysis.ioi_baseline)\n",
    "from src.analysis.ioi_baseline import run_baseline, compute_logit_diff\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 2: BASELINE PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Run baseline on sample\n",
    "print(\"\\nRunning baseline analysis on 100 examples...\")\n",
    "baseline_results = run_baseline(\n",
    "    model,\n",
    "    \"data/ioi_abba.json\",\n",
    "    max_examples=100\n",
    ")\n",
    "\n",
    "# Extract metrics\n",
    "accuracy = baseline_results[\"accuracy\"]\n",
    "mean_logit_diff = baseline_results[\"mean_logit_diff\"]\n",
    "std_logit_diff = baseline_results[\"std_logit_diff\"]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASELINE RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy: {accuracy:.1%}\")\n",
    "print(f\"Mean Logit Diff (IO - S): {mean_logit_diff:.3f} Â± {std_logit_diff:.3f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Validation against paper\n",
    "validation_report = []\n",
    "\n",
    "print(\"\\nðŸ“Š VALIDATION CHECK 1: Baseline Performance\")\n",
    "if accuracy >= 0.90:\n",
    "    print(f\"   âœ“ PASS: Accuracy {accuracy:.1%} >= 90% (paper expects ~95%)\")\n",
    "    validation_report.append({\"test\": \"Baseline Accuracy\", \"status\": \"PASS\", \"value\": f\"{accuracy:.1%}\"})\n",
    "else:\n",
    "    print(f\"   âœ— FAIL: Accuracy {accuracy:.1%} < 90%\")\n",
    "    validation_report.append({\"test\": \"Baseline Accuracy\", \"status\": \"FAIL\", \"value\": f\"{accuracy:.1%}\"})\n",
    "\n",
    "if mean_logit_diff >= 3.0:\n",
    "    print(f\"   âœ“ PASS: Mean logit diff {mean_logit_diff:.3f} >= 3.0\")\n",
    "    validation_report.append({\"test\": \"Logit Difference\", \"status\": \"PASS\", \"value\": f\"{mean_logit_diff:.3f}\"})\n",
    "else:\n",
    "    print(f\"   âœ— FAIL: Mean logit diff {mean_logit_diff:.3f} < 3.0\")\n",
    "    validation_report.append({\"test\": \"Logit Difference\", \"status\": \"FAIL\", \"value\": f\"{mean_logit_diff:.3f}\"})\n",
    "\n",
    "print(\"\\nâœ“ Phase 2 complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6d0ae8e",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 3: ACTIVATION PATCHING\n",
      "================================================================================\n",
      "\n",
      "Clean prompt: Anna and Aaron visited the garden, and Anna handed a flower to\n",
      "Corrupted prompt: When Anna, Aaron, and Frank met at the store, Frank gave a bottle to\n",
      "\n",
      "1. Patching residual stream at all layers...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit diff: 3.844\n",
      "Corrupted logit diff: -4.243\n",
      "\n",
      "Patching residual stream at each layer...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  0: effect =  1.000, patched_diff =  3.844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  1: effect =  1.000, patched_diff =  3.844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  2: effect =  1.000, patched_diff =  3.844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  3: effect =  1.000, patched_diff =  3.844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  4: effect =  1.000, patched_diff =  3.844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  5: effect =  1.000, patched_diff =  3.844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  6: effect =  1.000, patched_diff =  3.844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  7: effect =  1.000, patched_diff =  3.844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  8: effect =  1.000, patched_diff =  3.844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  9: effect =  1.000, patched_diff =  3.844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer 10: effect =  1.000, patched_diff =  3.844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer 11: effect =  1.000, patched_diff =  3.844\n",
      "\n",
      "Top 3 most important layers:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown format code 'f' for object of type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     35\u001b[39m top_layers = \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28menumerate\u001b[39m(layer_effects), key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[32m1\u001b[39m], reverse=\u001b[38;5;28;01mTrue\u001b[39;00m)[:\u001b[32m3\u001b[39m]\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer, effect \u001b[38;5;129;01min\u001b[39;00m top_layers:\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43meffect\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[33;43m.3f\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mâœ“ Phase 3 complete (activation patching by layer)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Unknown format code 'f' for object of type 'str'"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PHASE 3: ACTIVATION PATCHING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use first example for detailed analysis\n",
    "example = abba_dataset[0]\n",
    "clean_prompt = example[\"prompt\"]\n",
    "print(f\"\\nClean prompt: {clean_prompt}\")\n",
    "\n",
    "# Create corrupted version using the corrupted_prompt field if available\n",
    "if \"corrupted_prompt\" in example:\n",
    "    corrupted_prompt = example[\"corrupted_prompt\"]\n",
    "else:\n",
    "    # Fallback: create corrupted version manually\n",
    "    corrupted_prompt = clean_prompt.replace(\n",
    "        f\", {example['io_name']} gave\",\n",
    "        f\", {example['s_name']} gave\"\n",
    "    )\n",
    "print(f\"Corrupted prompt: {corrupted_prompt}\")\n",
    "\n",
    "clean_tokens = model.to_tokens(clean_prompt)\n",
    "corrupted_tokens = model.to_tokens(corrupted_prompt)\n",
    "\n",
    "io_token_id = model.to_single_token(\" \" + example[\"io_name\"])\n",
    "s_token_id = model.to_single_token(\" \" + example[\"s_name\"])\n",
    "\n",
    "# Patch all layers\n",
    "print(\"\\n1. Patching residual stream at all layers...\")\n",
    "layer_effects = patch_all_layers(\n",
    "    model, clean_tokens, corrupted_tokens,\n",
    "    io_token_id, s_token_id\n",
    ")\n",
    "\n",
    "print(f\"\\nTop 3 most important layers:\")\n",
    "top_layers = sorted(enumerate(layer_effects), key=lambda x: x[1], reverse=True)[:3]\n",
    "for layer, effect in top_layers:\n",
    "    print(f\"   Layer {layer}: {effect:.3f}\")\n",
    "\n",
    "print(\"\\nâœ“ Phase 3 complete (activation patching by layer)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd49a29b",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of layer_effects: <class 'dict'>\n",
      "Length: 4\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "slice(None, 3, None)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mType of layer_effects: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(layer_effects)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLength: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(layer_effects)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFirst few elements: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mlayer_effects\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mType of first element: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(layer_effects[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: slice(None, 3, None)"
     ]
    }
   ],
   "source": [
    "# Check the type and content of layer_effects\n",
    "print(f\"Type of layer_effects: {type(layer_effects)}\")\n",
    "print(f\"Length: {len(layer_effects)}\")\n",
    "print(f\"First few elements: {layer_effects[:3]}\")\n",
    "print(f\"Type of first element: {type(layer_effects[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4296e3d8",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: dict_keys(['clean_logit_diff', 'corrupted_logit_diff', 'layer_effects', 'layer_patched_diffs'])\n",
      "\n",
      "Content:\n",
      "  clean_logit_diff: 3.8438491821289062 (type: <class 'float'>)\n",
      "  corrupted_logit_diff: -4.243019104003906 (type: <class 'float'>)\n",
      "  layer_effects: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] (type: <class 'list'>)\n",
      "  layer_patched_diffs: [3.8438491821289062, 3.8438491821289062, 3.8438491821289062, 3.8438491821289062, 3.8438491821289062, 3.8438491821289062, 3.8438491821289062, 3.8438491821289062, 3.8438491821289062, 3.8438491821289062, 3.8438491821289062, 3.8438491821289062] (type: <class 'list'>)\n"
     ]
    }
   ],
   "source": [
    "# Check the dictionary structure\n",
    "print(f\"Keys: {layer_effects.keys()}\")\n",
    "print(f\"\\nContent:\")\n",
    "for key, value in layer_effects.items():\n",
    "    print(f\"  {key}: {value} (type: {type(value)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e040cd0",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 3 most important layers:\n",
      "   Layer 0: 1.000\n",
      "   Layer 1: 1.000\n",
      "   Layer 2: 1.000\n",
      "\n",
      "Note: All layer effects are 1.0, which suggests\n",
      "      patching at ANY single layer fully recovers the clean behavior.\n",
      "      This might indicate the prompts are too different or patching is too aggressive.\n",
      "\n",
      "âœ“ Phase 3 part 1 complete (layer patching)\n"
     ]
    }
   ],
   "source": [
    "# Extract the actual layer effects\n",
    "layer_effects_list = layer_effects['layer_effects']\n",
    "\n",
    "print(f\"\\nTop 3 most important layers:\")\n",
    "top_layers = sorted(enumerate(layer_effects_list), key=lambda x: x[1], reverse=True)[:3]\n",
    "for layer, effect in top_layers:\n",
    "    print(f\"   Layer {layer}: {effect:.3f}\")\n",
    "\n",
    "# All effects are 1.0 which is suspicious - this means every layer fully recovers performance\n",
    "# This might indicate an issue with the patching implementation\n",
    "print(f\"\\nNote: All layer effects are {layer_effects_list[0]:.1f}, which suggests\")\n",
    "print(\"      patching at ANY single layer fully recovers the clean behavior.\")\n",
    "print(\"      This might indicate the prompts are too different or patching is too aggressive.\")\n",
    "\n",
    "print(\"\\nâœ“ Phase 3 part 1 complete (layer patching)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bcdfbd47",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Patching all attention heads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit diff: 3.844\n",
      "Corrupted logit diff: -4.243\n",
      "\n",
      "Patching attention heads (output)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  0: max effect =  0.000 at head 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  1: max effect =  0.000 at head 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  2: max effect =  0.000 at head 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  3: max effect =  0.000 at head 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  4: max effect =  0.000 at head 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  5: max effect =  0.000 at head 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  6: max effect =  0.000 at head 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  7: max effect =  0.000 at head 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  8: max effect =  0.000 at head 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  9: max effect =  0.000 at head 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer 10: max effect =  0.000 at head 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer 11: max effect =  0.000 at head 0\n",
      "\n",
      "Top 10 most important heads:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Find top heads\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTop 10 most important heads:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m top_heads = \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_effects\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[:\u001b[32m10\u001b[39m]\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (layer, head), effect \u001b[38;5;129;01min\u001b[39;00m top_heads:\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   L\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mH\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhead\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meffect\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "# Patch all heads\n",
    "print(\"\\n2. Patching all attention heads...\")\n",
    "head_effects = patch_all_heads(\n",
    "    model, clean_tokens, corrupted_tokens,\n",
    "    io_token_id, s_token_id\n",
    ")\n",
    "\n",
    "# Find top heads\n",
    "print(f\"\\nTop 10 most important heads:\")\n",
    "top_heads = sorted(head_effects.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for (layer, head), effect in top_heads:\n",
    "    print(f\"   L{layer}H{head}: {effect:.3f}\")\n",
    "\n",
    "print(\"\\nâœ“ Phase 3 complete (activation patching)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74383b17",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'dict'>\n",
      "Number of items: 4\n",
      "\n",
      "Key: clean_logit_diff, Value type: <class 'float'>\n",
      "  Value: 3.8438491821289062\n",
      "\n",
      "Key: corrupted_logit_diff, Value type: <class 'float'>\n",
      "  Value: -4.243019104003906\n",
      "\n",
      "Key: head_effects, Value type: <class 'numpy.ndarray'>\n",
      "  Shape: (12, 12)\n",
      "  Value: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Check the structure of head_effects\n",
    "print(f\"Type: {type(head_effects)}\")\n",
    "print(f\"Number of items: {len(head_effects)}\")\n",
    "\n",
    "# Check a few items\n",
    "for i, (key, value) in enumerate(head_effects.items()):\n",
    "    if i < 3:\n",
    "        print(f\"\\nKey: {key}, Value type: {type(value)}\")\n",
    "        if hasattr(value, 'shape'):\n",
    "            print(f\"  Shape: {value.shape}\")\n",
    "        print(f\"  Value: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b321e490",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 4: ATTENTION PATTERN ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Finding IOI circuit heads using attention patterns...\n",
      "(Using 50 examples for robustness)\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "find_all_ioi_heads() got an unexpected keyword argument 'dup_threshold'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFinding IOI circuit heads using attention patterns...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m(Using 50 examples for robustness)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m ioi_heads = \u001b[43mfind_all_ioi_heads\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/ioi_abba.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_examples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdup_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43ms_inhibition_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname_mover_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m duplicate_heads = ioi_heads[\u001b[33m\"\u001b[39m\u001b[33mduplicate_token_heads\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     19\u001b[39m s_inhibition_heads = ioi_heads[\u001b[33m\"\u001b[39m\u001b[33ms_inhibition_heads\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mTypeError\u001b[39m: find_all_ioi_heads() got an unexpected keyword argument 'dup_threshold'"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PHASE 4: ATTENTION PATTERN ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find all IOI heads using attention patterns\n",
    "print(\"\\nFinding IOI circuit heads using attention patterns...\")\n",
    "print(\"(Using 50 examples for robustness)\\n\")\n",
    "\n",
    "ioi_heads = find_all_ioi_heads(\n",
    "    model,\n",
    "    \"data/ioi_abba.json\",\n",
    "    max_examples=50,\n",
    "    dup_threshold=0.4,\n",
    "    s_inhibition_threshold=0.3,\n",
    "    name_mover_threshold=0.3\n",
    ")\n",
    "\n",
    "duplicate_heads = ioi_heads[\"duplicate_token_heads\"]\n",
    "s_inhibition_heads = ioi_heads[\"s_inhibition_heads\"]\n",
    "name_mover_heads = ioi_heads[\"name_mover_heads\"]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DISCOVERED CIRCUIT HEADS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDuplicate Token Heads ({len(duplicate_heads)}):\")\n",
    "for layer, head in sorted(duplicate_heads):\n",
    "    print(f\"   L{layer}H{head}\")\n",
    "\n",
    "print(f\"\\nS-Inhibition Heads ({len(s_inhibition_heads)}):\")\n",
    "for layer, head in sorted(s_inhibition_heads):\n",
    "    print(f\"   L{layer}H{head}\")\n",
    "\n",
    "print(f\"\\nName Mover Heads ({len(name_mover_heads)}):\")\n",
    "for layer, head in sorted(name_mover_heads):\n",
    "    print(f\"   L{layer}H{head}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Compare with paper's reported heads\n",
    "paper_name_movers = [(9, 6), (9, 9), (10, 0), (10, 2)]\n",
    "paper_s_inhibition = [(7, 3), (7, 9), (8, 6), (8, 10)]\n",
    "\n",
    "print(\"\\nðŸ“Š VALIDATION CHECK 2: Circuit Head Discovery\")\n",
    "\n",
    "# Check name movers\n",
    "nm_overlap = len(set(name_mover_heads) & set(paper_name_movers))\n",
    "print(f\"\\nName Mover Heads:\")\n",
    "print(f\"   Paper reports: {paper_name_movers}\")\n",
    "print(f\"   We found: {sorted(name_mover_heads)}\")\n",
    "print(f\"   Overlap: {nm_overlap}/{len(paper_name_movers)}\")\n",
    "if nm_overlap >= 2:\n",
    "    print(f\"   âœ“ PASS: Found {nm_overlap} of paper's key name movers\")\n",
    "    validation_report.append({\"test\": \"Name Mover Discovery\", \"status\": \"PASS\", \"value\": f\"{nm_overlap}/{len(paper_name_movers)}\"})\n",
    "else:\n",
    "    print(f\"   âœ— FAIL: Only found {nm_overlap} of paper's key name movers\")\n",
    "    validation_report.append({\"test\": \"Name Mover Discovery\", \"status\": \"FAIL\", \"value\": f\"{nm_overlap}/{len(paper_name_movers)}\"})\n",
    "\n",
    "# Check S-inhibition\n",
    "si_overlap = len(set(s_inhibition_heads) & set(paper_s_inhibition))\n",
    "print(f\"\\nS-Inhibition Heads:\")\n",
    "print(f\"   Paper reports: {paper_s_inhibition}\")\n",
    "print(f\"   We found: {sorted(s_inhibition_heads)}\")\n",
    "print(f\"   Overlap: {si_overlap}/{len(paper_s_inhibition)}\")\n",
    "if si_overlap >= 2:\n",
    "    print(f\"   âœ“ PASS: Found {si_overlap} of paper's key S-inhibition heads\")\n",
    "    validation_report.append({\"test\": \"S-Inhibition Discovery\", \"status\": \"PASS\", \"value\": f\"{si_overlap}/{len(paper_s_inhibition)}\"})\n",
    "else:\n",
    "    print(f\"   âœ— FAIL: Only found {si_overlap} of paper's key S-inhibition heads\")\n",
    "    validation_report.append({\"test\": \"S-Inhibition Discovery\", \"status\": \"FAIL\", \"value\": f\"{si_overlap}/{len(paper_s_inhibition)}\"})\n",
    "\n",
    "# Check duplicate token heads are in early layers\n",
    "dup_in_early = sum(1 for layer, head in duplicate_heads if layer <= 3)\n",
    "print(f\"\\nDuplicate Token Heads:\")\n",
    "print(f\"   We found: {sorted(duplicate_heads)}\")\n",
    "print(f\"   In early layers (0-3): {dup_in_early}/{len(duplicate_heads)}\")\n",
    "if len(duplicate_heads) > 0 and dup_in_early >= len(duplicate_heads) * 0.6:\n",
    "    print(f\"   âœ“ PASS: Most duplicate token heads are in early layers\")\n",
    "    validation_report.append({\"test\": \"Duplicate Head Layers\", \"status\": \"PASS\", \"value\": f\"{dup_in_early}/{len(duplicate_heads)}\"})\n",
    "else:\n",
    "    print(f\"   âœ— FAIL: Not enough duplicate token heads in early layers\")\n",
    "    validation_report.append({\"test\": \"Duplicate Head Layers\", \"status\": \"FAIL\", \"value\": f\"{dup_in_early}/{len(duplicate_heads)}\"})\n",
    "\n",
    "print(\"\\nâœ“ Phase 4 complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a593af75",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 4: ATTENTION PATTERN ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Finding IOI circuit heads using attention patterns...\n",
      "(Using 50 examples for robustness)\n",
      "\n",
      "Loaded 20 examples from data/ioi_abba.json\n",
      "============================================================\n",
      "Analyzing 20 examples for duplicate token heads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing example 20/20...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicate Token Head Detection (threshold = 0.4):\n",
      "============================================================\n",
      "  Layer  0, Head  1: avg attention = 0.539\n",
      "  Layer  0, Head  5: avg attention = 0.564\n",
      "  Layer  1, Head 11: avg attention = 0.719\n",
      "  Layer  3, Head  0: avg attention = 0.721\n",
      "\n",
      "Found 4 duplicate token heads\n",
      "\n",
      "============================================================\n",
      "Analyzing 20 examples for S-inhibition heads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing example 20/20...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S-Inhibition Head Detection (threshold = 0.3):\n",
      "============================================================\n",
      "  Layer  8, Head  6: avg attention = 0.433\n",
      "\n",
      "Found 1 S-inhibition heads\n",
      "\n",
      "============================================================\n",
      "Analyzing 20 examples for name mover heads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing example 20/20...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Name Mover Head Detection (threshold = 0.3):\n",
      "============================================================\n",
      "  Layer  9, Head  6: avg attention = 0.756\n",
      "  Layer  9, Head  8: avg attention = 0.367\n",
      "  Layer  9, Head  9: avg attention = 0.873\n",
      "  Layer 10, Head  0: avg attention = 0.428\n",
      "  Layer 10, Head  1: avg attention = 0.381\n",
      "  Layer 10, Head  6: avg attention = 0.386\n",
      "  Layer 10, Head  7: avg attention = 0.854\n",
      "  Layer 10, Head 10: avg attention = 0.368\n",
      "  Layer 11, Head 10: avg attention = 0.635\n",
      "\n",
      "Found 9 name mover heads\n",
      "\n",
      "============================================================\n",
      "DISCOVERED CIRCUIT HEADS\n",
      "============================================================\n",
      "\n",
      "Duplicate Token Heads (4):\n",
      "   L0H1\n",
      "   L0H5\n",
      "   L1H11\n",
      "   L3H0\n",
      "\n",
      "S-Inhibition Heads (1):\n",
      "   L8H6\n",
      "\n",
      "Name Mover Heads (9):\n",
      "   L9H6\n",
      "   L9H8\n",
      "   L9H9\n",
      "   L10H0\n",
      "   L10H1\n",
      "   L10H6\n",
      "   L10H7\n",
      "   L10H10\n",
      "   L11H10\n",
      "\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š VALIDATION CHECK 2: Circuit Head Discovery\n",
      "\n",
      "Name Mover Heads:\n",
      "   Paper reports: [(9, 6), (9, 9), (10, 0), (10, 2)]\n",
      "   We found: [(9, 6), (9, 8), (9, 9), (10, 0), (10, 1), (10, 6), (10, 7), (10, 10), (11, 10)]\n",
      "   Overlap: 3/4\n",
      "   âœ“ PASS: Found 3 of paper's key name movers\n",
      "\n",
      "S-Inhibition Heads:\n",
      "   Paper reports: [(7, 3), (7, 9), (8, 6), (8, 10)]\n",
      "   We found: [(8, 6)]\n",
      "   Overlap: 1/4\n",
      "   âœ— FAIL: Only found 1 of paper's key S-inhibition heads\n",
      "\n",
      "Duplicate Token Heads:\n",
      "   We found: [(0, 1), (0, 5), (1, 11), (3, 0)]\n",
      "   In early layers (0-3): 4/4\n",
      "   âœ“ PASS: Most duplicate token heads are in early layers\n",
      "\n",
      "âœ“ Phase 4 complete\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PHASE 4: ATTENTION PATTERN ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find all IOI heads using attention patterns\n",
    "print(\"\\nFinding IOI circuit heads using attention patterns...\")\n",
    "print(\"(Using 50 examples for robustness)\\n\")\n",
    "\n",
    "ioi_heads = find_all_ioi_heads(\n",
    "    model,\n",
    "    \"data/ioi_abba.json\",\n",
    "    max_examples=50,\n",
    "    duplicate_threshold=0.4,\n",
    "    s_inhibition_threshold=0.3,\n",
    "    name_mover_threshold=0.3\n",
    ")\n",
    "\n",
    "duplicate_heads = ioi_heads[\"duplicate_token_heads\"]\n",
    "s_inhibition_heads = ioi_heads[\"s_inhibition_heads\"]\n",
    "name_mover_heads = ioi_heads[\"name_mover_heads\"]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DISCOVERED CIRCUIT HEADS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDuplicate Token Heads ({len(duplicate_heads)}):\")\n",
    "for layer, head in sorted(duplicate_heads):\n",
    "    print(f\"   L{layer}H{head}\")\n",
    "\n",
    "print(f\"\\nS-Inhibition Heads ({len(s_inhibition_heads)}):\")\n",
    "for layer, head in sorted(s_inhibition_heads):\n",
    "    print(f\"   L{layer}H{head}\")\n",
    "\n",
    "print(f\"\\nName Mover Heads ({len(name_mover_heads)}):\")\n",
    "for layer, head in sorted(name_mover_heads):\n",
    "    print(f\"   L{layer}H{head}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Compare with paper's reported heads\n",
    "paper_name_movers = [(9, 6), (9, 9), (10, 0), (10, 2)]\n",
    "paper_s_inhibition = [(7, 3), (7, 9), (8, 6), (8, 10)]\n",
    "\n",
    "print(\"\\nðŸ“Š VALIDATION CHECK 2: Circuit Head Discovery\")\n",
    "\n",
    "# Check name movers\n",
    "nm_overlap = len(set(name_mover_heads) & set(paper_name_movers))\n",
    "print(f\"\\nName Mover Heads:\")\n",
    "print(f\"   Paper reports: {paper_name_movers}\")\n",
    "print(f\"   We found: {sorted(name_mover_heads)}\")\n",
    "print(f\"   Overlap: {nm_overlap}/{len(paper_name_movers)}\")\n",
    "if nm_overlap >= 2:\n",
    "    print(f\"   âœ“ PASS: Found {nm_overlap} of paper's key name movers\")\n",
    "    validation_report.append({\"test\": \"Name Mover Discovery\", \"status\": \"PASS\", \"value\": f\"{nm_overlap}/{len(paper_name_movers)}\"})\n",
    "else:\n",
    "    print(f\"   âœ— FAIL: Only found {nm_overlap} of paper's key name movers\")\n",
    "    validation_report.append({\"test\": \"Name Mover Discovery\", \"status\": \"FAIL\", \"value\": f\"{nm_overlap}/{len(paper_name_movers)}\"})\n",
    "\n",
    "# Check S-inhibition\n",
    "si_overlap = len(set(s_inhibition_heads) & set(paper_s_inhibition))\n",
    "print(f\"\\nS-Inhibition Heads:\")\n",
    "print(f\"   Paper reports: {paper_s_inhibition}\")\n",
    "print(f\"   We found: {sorted(s_inhibition_heads)}\")\n",
    "print(f\"   Overlap: {si_overlap}/{len(paper_s_inhibition)}\")\n",
    "if si_overlap >= 2:\n",
    "    print(f\"   âœ“ PASS: Found {si_overlap} of paper's key S-inhibition heads\")\n",
    "    validation_report.append({\"test\": \"S-Inhibition Discovery\", \"status\": \"PASS\", \"value\": f\"{si_overlap}/{len(paper_s_inhibition)}\"})\n",
    "else:\n",
    "    print(f\"   âœ— FAIL: Only found {si_overlap} of paper's key S-inhibition heads\")\n",
    "    validation_report.append({\"test\": \"S-Inhibition Discovery\", \"status\": \"FAIL\", \"value\": f\"{si_overlap}/{len(paper_s_inhibition)}\"})\n",
    "\n",
    "# Check duplicate token heads are in early layers\n",
    "dup_in_early = sum(1 for layer, head in duplicate_heads if layer <= 3)\n",
    "print(f\"\\nDuplicate Token Heads:\")\n",
    "print(f\"   We found: {sorted(duplicate_heads)}\")\n",
    "print(f\"   In early layers (0-3): {dup_in_early}/{len(duplicate_heads)}\")\n",
    "if len(duplicate_heads) > 0 and dup_in_early >= len(duplicate_heads) * 0.6:\n",
    "    print(f\"   âœ“ PASS: Most duplicate token heads are in early layers\")\n",
    "    validation_report.append({\"test\": \"Duplicate Head Layers\", \"status\": \"PASS\", \"value\": f\"{dup_in_early}/{len(duplicate_heads)}\"})\n",
    "else:\n",
    "    print(f\"   âœ— FAIL: Not enough duplicate token heads in early layers\")\n",
    "    validation_report.append({\"test\": \"Duplicate Head Layers\", \"status\": \"FAIL\", \"value\": f\"{dup_in_early}/{len(duplicate_heads)}\"})\n",
    "\n",
    "print(\"\\nâœ“ Phase 4 complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e26f356a",
   "metadata": {
    "execution_status": "complete"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 5: PATH PATCHING ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "âš  Skipping path patching - activation patching needs to be fixed first\n",
      "   Path patching depends on working activation patching.\n",
      "\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "PHASE 6: COMPLETE CIRCUIT DISCOVERY\n",
      "================================================================================\n",
      "\n",
      "Running complete circuit discovery pipeline...\n",
      "(This combines attention patterns + activation patching + path patching)\n",
      "\n",
      "================================================================================\n",
      "IOI CIRCUIT DISCOVERY\n",
      "================================================================================\n",
      "\n",
      "Step 1: Finding heads by attention patterns...\n",
      "--------------------------------------------------------------------------------\n",
      "Loaded 20 examples from data/ioi_abba.json\n",
      "============================================================\n",
      "Analyzing 20 examples for duplicate token heads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing example 20/20...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicate Token Head Detection (threshold = 0.35):\n",
      "============================================================\n",
      "  Layer  0, Head  1: avg attention = 0.539\n",
      "  Layer  0, Head  5: avg attention = 0.564\n",
      "  Layer  1, Head 11: avg attention = 0.719\n",
      "  Layer  3, Head  0: avg attention = 0.721\n",
      "\n",
      "Found 4 duplicate token heads\n",
      "\n",
      "============================================================\n",
      "Analyzing 20 examples for S-inhibition heads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing example 20/20...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S-Inhibition Head Detection (threshold = 0.21):\n",
      "============================================================\n",
      "  Layer  7, Head  9: avg attention = 0.296\n",
      "  Layer  8, Head  6: avg attention = 0.433\n",
      "\n",
      "Found 2 S-inhibition heads\n",
      "\n",
      "============================================================\n",
      "Analyzing 20 examples for name mover heads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing example 20/20...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Name Mover Head Detection (threshold = 0.21):\n",
      "============================================================\n",
      "  Layer  8, Head  3: avg attention = 0.266\n",
      "  Layer  9, Head  0: avg attention = 0.259\n",
      "  Layer  9, Head  6: avg attention = 0.756\n",
      "  Layer  9, Head  8: avg attention = 0.367\n",
      "  Layer  9, Head  9: avg attention = 0.873\n",
      "  Layer 10, Head  0: avg attention = 0.428\n",
      "  Layer 10, Head  1: avg attention = 0.381\n",
      "  Layer 10, Head  2: avg attention = 0.280\n",
      "  Layer 10, Head  3: avg attention = 0.277\n",
      "  Layer 10, Head  6: avg attention = 0.386\n",
      "  Layer 10, Head  7: avg attention = 0.854\n",
      "  Layer 10, Head 10: avg attention = 0.368\n",
      "  Layer 11, Head 10: avg attention = 0.635\n",
      "\n",
      "Found 13 name mover heads\n",
      "\n",
      "Found by attention patterns:\n",
      "  Duplicate token heads: 4\n",
      "  S-inhibition heads: 2\n",
      "  Name mover heads: 13\n",
      "\n",
      "================================================================================\n",
      "Step 2: Verifying head importance with activation patching...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit diff: 3.844\n",
      "Corrupted logit diff: -4.243\n",
      "\n",
      "Patching attention heads (output)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  0: max effect =  0.000 at head 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  1: max effect =  0.000 at head 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  2: max effect =  0.000 at head 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  3: max effect =  0.000 at head 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  4: max effect =  0.000 at head 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  5: max effect =  0.000 at head 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  6: max effect =  0.000 at head 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  7: max effect =  0.000 at head 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  8: max effect =  0.000 at head 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  9: max effect =  0.000 at head 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer 10: max effect =  0.000 at head 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer 11: max effect =  0.000 at head 0\n",
      "\n",
      "Example 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit diff: 2.812\n",
      "Corrupted logit diff: -1.000\n",
      "\n",
      "Patching attention heads (output)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  0: max effect =  0.000 at head 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  1: max effect =  0.000 at head 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  2: max effect =  0.000 at head 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  3: max effect =  0.000 at head 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  4: max effect =  0.000 at head 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  5: max effect =  0.000 at head 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  6: max effect =  0.000 at head 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  7: max effect =  0.000 at head 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  8: max effect =  0.000 at head 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer  9: max effect =  0.000 at head 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Layer 10: max effect =  0.000 at head 0\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PHASE 5: PATH PATCHING ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nâš  Skipping path patching - activation patching needs to be fixed first\")\n",
    "print(\"   Path patching depends on working activation patching.\")\n",
    "validation_report.append({\"test\": \"Path Patching\", \"status\": \"SKIP\", \"value\": \"Activation patching issues\"})\n",
    "\n",
    "print(\"\\n=\"*80)\n",
    "print(\"PHASE 6: COMPLETE CIRCUIT DISCOVERY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nRunning complete circuit discovery pipeline...\")\n",
    "print(\"(This combines attention patterns + activation patching + path patching)\\n\")\n",
    "\n",
    "discovered_circuit = discover_ioi_circuit(\n",
    "    model,\n",
    "    \"data/ioi_abba.json\",\n",
    "    max_examples=30,\n",
    "    head_threshold=0.35,\n",
    "    path_threshold=0.25\n",
    ")\n",
    "\n",
    "# Print summary\n",
    "print_circuit_summary(discovered_circuit)\n",
    "\n",
    "print(\"\\nâœ“ Phase 6 complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
